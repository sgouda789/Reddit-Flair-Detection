{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Reddit's API, PRAW to scrap subreddits, and Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising the Reddit object with user's credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id = \"#\", \n",
    "                     client_secret = \"#\", \n",
    "                     user_agent = \"#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the subreddit of r/india"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subred = reddit.subreddit('india')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considering \"hot\", \"top\", \"new\" and \"controversial\" filters to scrap subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot = subred.hot(limit=3000)\n",
    "top = subred.top(limit=3000)\n",
    "controversial = subred.controversial(limit=3000)\n",
    "new = subred.new(limit=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "praw.models.listing.generator.ListingGenerator"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initilizing a dictionary \"data\" to store the scrapped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"id\":[], \"url\":[],\"title\":[],  \"body\":[], \"flair\":[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data from corresponding filtered subreddits and appending in \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in hot:\n",
    "    \n",
    "    data['id'].append(i.id)\n",
    "    data['url'].append(i.url)\n",
    "    data['title'].append(i.title)\n",
    "    data['body'].append(i.selftext)    \n",
    "    data['flair'].append(i.link_flair_text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in top:\n",
    "    \n",
    "    data['id'].append(i.id)\n",
    "    data['url'].append(i.url)\n",
    "    data['title'].append(i.title)\n",
    "    data['body'].append(i.selftext)    \n",
    "    data['flair'].append(i.link_flair_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in controversial:\n",
    "    \n",
    "    data['id'].append(i.id)\n",
    "    data['url'].append(i.url)\n",
    "    data['title'].append(i.title)\n",
    "    data['body'].append(i.selftext)    \n",
    "    data['flair'].append(i.link_flair_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in new:\n",
    "    \n",
    "    data['id'].append(i.id)\n",
    "    data['url'].append(i.url)\n",
    "    data['title'].append(i.title)\n",
    "    data['body'].append(i.selftext)    \n",
    "    data['flair'].append(i.link_flair_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the dictionary to a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>flair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g1zi21</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/g1zi21...</td>\n",
       "      <td>Coronavirus (COVID-19) Megathread - News and U...</td>\n",
       "      <td>###[Covid-19 Fundraisers &amp; Donation Links](htt...</td>\n",
       "      <td>Coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g4d2ix</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/g4d2ix...</td>\n",
       "      <td>[Monthly Happiness Thread] Randians, please sh...</td>\n",
       "      <td>&lt;3         \\n          \\nLinks:               ...</td>\n",
       "      <td>Scheduled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g6z6ly</td>\n",
       "      <td>https://i.redd.it/8l3vwdax2ou41.jpg</td>\n",
       "      <td>Re-Creation Of Humanity (artist : Hasif khan)</td>\n",
       "      <td></td>\n",
       "      <td>Non-Political</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g70l6e</td>\n",
       "      <td>https://www.nationalheraldindia.com/internatio...</td>\n",
       "      <td>Gulf News editor in Dubai receives threats fro...</td>\n",
       "      <td></td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>g73kri</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/g73kri...</td>\n",
       "      <td>Judiciary of India has never been this bad</td>\n",
       "      <td>CAA-NRC - \\n\\nWe will hear petition next month...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                                url  \\\n",
       "0  g1zi21  https://www.reddit.com/r/india/comments/g1zi21...   \n",
       "1  g4d2ix  https://www.reddit.com/r/india/comments/g4d2ix...   \n",
       "2  g6z6ly                https://i.redd.it/8l3vwdax2ou41.jpg   \n",
       "3  g70l6e  https://www.nationalheraldindia.com/internatio...   \n",
       "4  g73kri  https://www.reddit.com/r/india/comments/g73kri...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Coronavirus (COVID-19) Megathread - News and U...   \n",
       "1  [Monthly Happiness Thread] Randians, please sh...   \n",
       "2      Re-Creation Of Humanity (artist : Hasif khan)   \n",
       "3  Gulf News editor in Dubai receives threats fro...   \n",
       "4         Judiciary of India has never been this bad   \n",
       "\n",
       "                                                body          flair  \n",
       "0  ###[Covid-19 Fundraisers & Donation Links](htt...    Coronavirus  \n",
       "1  <3         \\n          \\nLinks:               ...      Scheduled  \n",
       "2                                                     Non-Political  \n",
       "3                                                          Politics  \n",
       "4  CAA-NRC - \\n\\nWe will hear petition next month...       Politics  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3632, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since one subreddit can appear in more than one filter, the DF is sorted and duplicate elements are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(\"id\", inplace = True)\n",
    "df.drop_duplicates(subset =\"id\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2859, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the unique flairs and their frequencies from the collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Politics                          977\n",
       "Non-Political                     772\n",
       "Coronavirus                       353\n",
       "AskIndia                          177\n",
       "Policy/Economy                     88\n",
       "[R]eddiquette                      71\n",
       "Business/Finance                   58\n",
       "Photography                        53\n",
       "Sports                             42\n",
       "Science/Technology                 25\n",
       "Food                               21\n",
       "Unverified                         20\n",
       "Scheduled                          12\n",
       "CAA-NRC                            10\n",
       "Moderated                           7\n",
       "Misleading                          6\n",
       "Policy                              4\n",
       "CAA-NRC-NPR                         4\n",
       "r/all                               3\n",
       "Policy & Economy                    3\n",
       "Demonetization                      3\n",
       "Entertainment                       3\n",
       "/r/all                              2\n",
       "AMA                                 2\n",
       "Science & Technology                1\n",
       "TIL                                 1\n",
       "Policy/Economy -2017 Article        1\n",
       "Lifehacks                           1\n",
       "40 Martyrs                          1\n",
       "Politics [OLD]                      1\n",
       "Politics -- Source in comments      1\n",
       "Totally real                        1\n",
       "Meta                                1\n",
       "Official Sadness Thread             1\n",
       "Zoke Tyme                           1\n",
       "Original Comics                     1\n",
       "AMA Concluded                       1\n",
       "On Internet Shutdowns               1\n",
       "Original Content :)                 1\n",
       "Goal Achieved!!!                    1\n",
       "OC                                  1\n",
       "[Editorialised]                     1\n",
       "Politics [Megathread]               1\n",
       "Misleading Headline                 1\n",
       "Name: flair, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['flair'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the top 10 flairs from the scrapped list as the categories for the succeeding prediction task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_flairs = [\"Politics\", \"Non-Political\", \"Coronavirus\", \"AskIndia\", \"Policy/Economy\", \"[R]eddiquette\", \n",
    "              \"Photography\", \"Business/Finance\", \"Sports\", \"Science/Technology\",\"Food\"]\n",
    "\n",
    "df_top = df.loc[df['flair'].isin(top_flairs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2637, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>flair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2348</th>\n",
       "      <td>1vfqzr</td>\n",
       "      <td>http://www.bollywoodmantra.com/news/hrithik-ro...</td>\n",
       "      <td>Hrithik Roshan to tie the knot for the second ...</td>\n",
       "      <td></td>\n",
       "      <td>Non-Political</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2167</th>\n",
       "      <td>1vgb3k</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/1vgb3k...</td>\n",
       "      <td>Girls of /r/India, would any of you be interes...</td>\n",
       "      <td>.</td>\n",
       "      <td>Non-Political</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2316</th>\n",
       "      <td>1vqu7l</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/1vqu7l...</td>\n",
       "      <td>Regarding the ongoing Kejru dramabaaz</td>\n",
       "      <td>I spoke with some people in Delhi , one of who...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2279</th>\n",
       "      <td>1w4xsp</td>\n",
       "      <td>http://bjpscams.com/</td>\n",
       "      <td>A site that lists out scams by the BJP</td>\n",
       "      <td></td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>1wcra7</td>\n",
       "      <td>http://www.truthofgujarat.com/modi-says-india-...</td>\n",
       "      <td>Modi says India has no War Memorials, here's a...</td>\n",
       "      <td></td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                                url  \\\n",
       "2348  1vfqzr  http://www.bollywoodmantra.com/news/hrithik-ro...   \n",
       "2167  1vgb3k  https://www.reddit.com/r/india/comments/1vgb3k...   \n",
       "2316  1vqu7l  https://www.reddit.com/r/india/comments/1vqu7l...   \n",
       "2279  1w4xsp                               http://bjpscams.com/   \n",
       "2139  1wcra7  http://www.truthofgujarat.com/modi-says-india-...   \n",
       "\n",
       "                                                  title  \\\n",
       "2348  Hrithik Roshan to tie the knot for the second ...   \n",
       "2167  Girls of /r/India, would any of you be interes...   \n",
       "2316              Regarding the ongoing Kejru dramabaaz   \n",
       "2279             A site that lists out scams by the BJP   \n",
       "2139  Modi says India has no War Memorials, here's a...   \n",
       "\n",
       "                                                   body          flair  \n",
       "2348                                                     Non-Political  \n",
       "2167                                                  .  Non-Political  \n",
       "2316  I spoke with some people in Delhi , one of who...       Politics  \n",
       "2279                                                          Politics  \n",
       "2139                                                          Politics  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As the data is skewed towards certain flairs and not others, we are scrapping again from the r/india subreddit for the above mentioned flairs with a limit of 100 for each flairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id = \"#\", \n",
    "                     client_secret = \"#\", \n",
    "                     user_agent = \"#\")\n",
    "\n",
    "subred = reddit.subreddit('india')\n",
    "sample_data = {\"id\":[], \"url\":[], \"title\":[], \"body\":[], \"flair\":[]}\n",
    "\n",
    "top_flairs = [\"Politics\", \"Non-Political\", \"Coronavirus\", \"AskIndia\", \"Policy/Economy\", \"[R]eddiquette\", \n",
    "              \"Photography\", \"Business/Finance\", \"Sports\", \"Science/Technology\"]\n",
    "\n",
    "for flair in top_flairs:\n",
    "  \n",
    "  top_f = subred.search(flair, limit=1000)\n",
    "  \n",
    "  for i in top_f:\n",
    "    \n",
    "    sample_data[\"id\"].append(i.id)\n",
    "    sample_data[\"url\"].append(i.url)\n",
    "    sample_data[\"title\"].append(i.title)\n",
    "    sample_data[\"body\"].append(i.selftext)\n",
    "    sample_data[\"flair\"].append(flair)\n",
    "    \n",
    "sample = pd.DataFrame(sample_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>flair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g2ct57</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/g2ct57...</td>\n",
       "      <td>A polite request to all Indians here</td>\n",
       "      <td>I don't know if it is the same situation in ot...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>futac9</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/futac9...</td>\n",
       "      <td>Pitting a community against a political party ...</td>\n",
       "      <td>First of all let me start by saying it was stu...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ff8sth</td>\n",
       "      <td>https://i.redd.it/yjo9wpy38el41.jpg</td>\n",
       "      <td>A new political party gave a full front page a...</td>\n",
       "      <td></td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fpaj1w</td>\n",
       "      <td>https://theprint.in/india/hit-by-backlash-over...</td>\n",
       "      <td>Hit by backlash over posts on lack of medical ...</td>\n",
       "      <td></td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fxs1vy</td>\n",
       "      <td>https://www.timesnownews.com/india/article/pol...</td>\n",
       "      <td>Politics in the time of corona: WB CM question...</td>\n",
       "      <td></td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                                url  \\\n",
       "0  g2ct57  https://www.reddit.com/r/india/comments/g2ct57...   \n",
       "1  futac9  https://www.reddit.com/r/india/comments/futac9...   \n",
       "2  ff8sth                https://i.redd.it/yjo9wpy38el41.jpg   \n",
       "3  fpaj1w  https://theprint.in/india/hit-by-backlash-over...   \n",
       "4  fxs1vy  https://www.timesnownews.com/india/article/pol...   \n",
       "\n",
       "                                               title  \\\n",
       "0               A polite request to all Indians here   \n",
       "1  Pitting a community against a political party ...   \n",
       "2  A new political party gave a full front page a...   \n",
       "3  Hit by backlash over posts on lack of medical ...   \n",
       "4  Politics in the time of corona: WB CM question...   \n",
       "\n",
       "                                                body     flair  \n",
       "0  I don't know if it is the same situation in ot...  Politics  \n",
       "1  First of all let me start by saying it was stu...  Politics  \n",
       "2                                                     Politics  \n",
       "3                                                     Politics  \n",
       "4                                                     Politics  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Coronavirus           247\n",
       "Politics              247\n",
       "Business/Finance      233\n",
       "AskIndia              232\n",
       "Sports                231\n",
       "Photography           222\n",
       "Science/Technology    221\n",
       "Policy/Economy        220\n",
       "Non-Political         216\n",
       "[R]eddiquette          18\n",
       "Name: flair, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['flair'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is seen that \"[R]eddiquette\"  flair has very less samples. Even if we use the previously scrapped data and concatenate it, it wont be in the same scale as others, so we will drop that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_9 = [\"Politics\", \"Non-Political\", \"Coronavirus\", \"AskIndia\", \"Policy/Economy\", \n",
    "              \"Photography\", \"Business/Finance\", \"Sports\", \"Science/Technology\"]\n",
    "\n",
    "final = sample.loc[sample['flair'].isin(top_9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Coronavirus           247\n",
       "Politics              247\n",
       "Business/Finance      233\n",
       "AskIndia              232\n",
       "Sports                231\n",
       "Photography           222\n",
       "Science/Technology    221\n",
       "Policy/Economy        220\n",
       "Non-Political         216\n",
       "Name: flair, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final['flair'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keshav/anaconda3/envs/reddit/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/keshav/anaconda3/envs/reddit/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "final.sort_values(\"id\", inplace = True)\n",
    "final.drop_duplicates(subset =\"id\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Coronavirus           241\n",
       "Politics              241\n",
       "Sports                227\n",
       "AskIndia              226\n",
       "Photography           221\n",
       "Science/Technology    220\n",
       "Business/Finance      217\n",
       "Non-Political         213\n",
       "Policy/Economy        207\n",
       "Name: flair, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final['flair'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Converting the DataFrame into a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('Data/reddit-top-flairs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considering the top 9 flair category, the data is scrapped and ready for EDA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
