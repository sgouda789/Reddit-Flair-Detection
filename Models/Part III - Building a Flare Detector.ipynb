{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data/reddit-top-flairs-cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>flair</th>\n",
       "      <th>dirty_title</th>\n",
       "      <th>dirty_body</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>num_words_body</th>\n",
       "      <th>num_unique_words_title</th>\n",
       "      <th>num_unique_words_body</th>\n",
       "      <th>num_chars_title</th>\n",
       "      <th>num_chars_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1267x2</td>\n",
       "      <td>https://twitter.com/TheVijayMallya/status/2618...</td>\n",
       "      <td>dat tweet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Business/Finance</td>\n",
       "      <td>Dat Tweet</td>\n",
       "      <td>.</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12i7hb</td>\n",
       "      <td>http://www.kickstarter.com/projects/1077963256...</td>\n",
       "      <td>mom started kickstarter show early indian phot...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Photography</td>\n",
       "      <td>So my mom started a Kickstarter for her show o...</td>\n",
       "      <td>.</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13ehd8</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/13ehd8...</td>\n",
       "      <td>please consider buying book india 2020 exodus ...</td>\n",
       "      <td>friend cowritten book s excerpt serious descri...</td>\n",
       "      <td>Science/Technology</td>\n",
       "      <td>Please consider buying our book \"India 2020 --...</td>\n",
       "      <td>A friend and I have co-written this book:\\nhtt...</td>\n",
       "      <td>11</td>\n",
       "      <td>77</td>\n",
       "      <td>10</td>\n",
       "      <td>74</td>\n",
       "      <td>71</td>\n",
       "      <td>539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17294b</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/17294b...</td>\n",
       "      <td>us double standards</td>\n",
       "      <td>terrorists afghanistan attack usa usa goes mot...</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>Us double standards</td>\n",
       "      <td>Terrorists in Afghanistan attack USA. USA goes...</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17x8uw</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/17x8uw...</td>\n",
       "      <td>list india related sub reddits</td>\n",
       "      <td>since removal drop linking related sub reddits...</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>A list of India Related Sub Reddits</td>\n",
       "      <td>Since the removal of the drop down we had, lin...</td>\n",
       "      <td>5</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>134</td>\n",
       "      <td>30</td>\n",
       "      <td>1855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                                url  \\\n",
       "0  1267x2  https://twitter.com/TheVijayMallya/status/2618...   \n",
       "1  12i7hb  http://www.kickstarter.com/projects/1077963256...   \n",
       "2  13ehd8  https://www.reddit.com/r/india/comments/13ehd8...   \n",
       "3  17294b  https://www.reddit.com/r/india/comments/17294b...   \n",
       "4  17x8uw  https://www.reddit.com/r/india/comments/17x8uw...   \n",
       "\n",
       "                                               title  \\\n",
       "0                                          dat tweet   \n",
       "1  mom started kickstarter show early indian phot...   \n",
       "2  please consider buying book india 2020 exodus ...   \n",
       "3                                us double standards   \n",
       "4                     list india related sub reddits   \n",
       "\n",
       "                                                body               flair  \\\n",
       "0                                                NaN    Business/Finance   \n",
       "1                                                NaN         Photography   \n",
       "2  friend cowritten book s excerpt serious descri...  Science/Technology   \n",
       "3  terrorists afghanistan attack usa usa goes mot...            AskIndia   \n",
       "4  since removal drop linking related sub reddits...            AskIndia   \n",
       "\n",
       "                                         dirty_title  \\\n",
       "0                                          Dat Tweet   \n",
       "1  So my mom started a Kickstarter for her show o...   \n",
       "2  Please consider buying our book \"India 2020 --...   \n",
       "3                                Us double standards   \n",
       "4                A list of India Related Sub Reddits   \n",
       "\n",
       "                                          dirty_body  num_words_title  \\\n",
       "0                                                  .                2   \n",
       "1                                                  .                9   \n",
       "2  A friend and I have co-written this book:\\nhtt...               11   \n",
       "3  Terrorists in Afghanistan attack USA. USA goes...                3   \n",
       "4  Since the removal of the drop down we had, lin...                5   \n",
       "\n",
       "   num_words_body  num_unique_words_title  num_unique_words_body  \\\n",
       "0               0                       2                      0   \n",
       "1               0                       9                      0   \n",
       "2              77                      10                     74   \n",
       "3              17                       3                     12   \n",
       "4             264                       5                    134   \n",
       "\n",
       "   num_chars_title  num_chars_body  \n",
       "0                9               0  \n",
       "1               64               0  \n",
       "2               71             539  \n",
       "3               19             128  \n",
       "4               30            1855  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping unwanted columns and shuffling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>flair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>businessfinance sbi reduces term deposit rate...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Business/Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>indiascoronaviruscurvehasspikedupcasescouldrea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>fast well know science technological side coun...</td>\n",
       "      <td>presume users aware surroundings field technol...</td>\n",
       "      <td>Science/Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>way interact fellow upsc aspirants reddit</td>\n",
       "      <td>quora used goto place civil discussions questi...</td>\n",
       "      <td>Science/Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>mahindra ibm develop blockchain solution suppl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Business/Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>askindia tips new car driver</td>\n",
       "      <td>bought used car last week m practicing driving...</td>\n",
       "      <td>AskIndia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>nonpolitical given ban porn sites india reddi...</td>\n",
       "      <td>use alt account instagram reddit get fast nonv...</td>\n",
       "      <td>Non-Political</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>type sports shoe running</td>\n",
       "      <td>recently started running currently using spark...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>’ india olympic medals elite athletes</td>\n",
       "      <td>’ curious question coming american paki whose ...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>wrong india please share perception</td>\n",
       "      <td>’ deny backward india terms many things econom...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2013 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "441    businessfinance sbi reduces term deposit rate...   \n",
       "1774  indiascoronaviruscurvehasspikedupcasescouldrea...   \n",
       "83    fast well know science technological side coun...   \n",
       "419          way interact fellow upsc aspirants reddit    \n",
       "345   mahindra ibm develop blockchain solution suppl...   \n",
       "...                                                 ...   \n",
       "811                       askindia tips new car driver    \n",
       "937    nonpolitical given ban porn sites india reddi...   \n",
       "956                           type sports shoe running    \n",
       "1064              ’ india olympic medals elite athletes   \n",
       "1250               wrong india please share perception    \n",
       "\n",
       "                                                   body               flair  \n",
       "441                                                 NaN    Business/Finance  \n",
       "1774                                                NaN         Coronavirus  \n",
       "83    presume users aware surroundings field technol...  Science/Technology  \n",
       "419   quora used goto place civil discussions questi...  Science/Technology  \n",
       "345                                                 NaN    Business/Finance  \n",
       "...                                                 ...                 ...  \n",
       "811   bought used car last week m practicing driving...            AskIndia  \n",
       "937   use alt account instagram reddit get fast nonv...       Non-Political  \n",
       "956   recently started running currently using spark...              Sports  \n",
       "1064  ’ curious question coming american paki whose ...              Sports  \n",
       "1250  ’ deny backward india terms many things econom...              Sports  \n",
       "\n",
       "[2013 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(['dirty_title','dirty_body','num_words_body','num_unique_words_title'\n",
    "           ,'num_unique_words_body','num_chars_title','num_words_title','num_chars_body','url','id'], axis=1)\n",
    "\n",
    "data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>flair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dat tweet</td>\n",
       "      <td>.</td>\n",
       "      <td>Business/Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mom started kickstarter show early indian phot...</td>\n",
       "      <td>.</td>\n",
       "      <td>Photography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>please consider buying book india 2020 exodus ...</td>\n",
       "      <td>friend cowritten book s excerpt serious descri...</td>\n",
       "      <td>Science/Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>us double standards</td>\n",
       "      <td>terrorists afghanistan attack usa usa goes mot...</td>\n",
       "      <td>AskIndia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>list india related sub reddits</td>\n",
       "      <td>since removal drop linking related sub reddits...</td>\n",
       "      <td>AskIndia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                          dat tweet   \n",
       "1  mom started kickstarter show early indian phot...   \n",
       "2  please consider buying book india 2020 exodus ...   \n",
       "3                                us double standards   \n",
       "4                     list india related sub reddits   \n",
       "\n",
       "                                                body               flair  \n",
       "0                                                  .    Business/Finance  \n",
       "1                                                  .         Photography  \n",
       "2  friend cowritten book s excerpt serious descri...  Science/Technology  \n",
       "3  terrorists afghanistan attack usa usa goes mot...            AskIndia  \n",
       "4  since removal drop linking related sub reddits...            AskIndia  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"body\"].fillna(\".\", inplace = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_flairs = [\"Politics\", \"Non-Political\", \"Coronavirus\", \"AskIndia\", \"Policy/Economy\", \n",
    "              \"Photography\", \"Business/Finance\", \"Sports\", \"Science/Technology\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The columns of \"title\" and \"body\" are combined as one feature as \"input_features\" and \"cat\" stores the output label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = data[\"title\"] + \" \"+ data[\"body\"]\n",
    "data = data.assign(input_features = input_features)\n",
    "\n",
    "y = data.flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flair</th>\n",
       "      <th>input_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business/Finance</td>\n",
       "      <td>dat tweet .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Photography</td>\n",
       "      <td>mom started kickstarter show early indian phot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Science/Technology</td>\n",
       "      <td>please consider buying book india 2020 exodus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AskIndia</td>\n",
       "      <td>us double standards terrorists afghanistan att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AskIndia</td>\n",
       "      <td>list india related sub reddits since removal d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                flair                                     input_features\n",
       "0    Business/Finance                                        dat tweet .\n",
       "1         Photography  mom started kickstarter show early indian phot...\n",
       "2  Science/Technology  please consider buying book india 2020 exodus ...\n",
       "3            AskIndia  us double standards terrorists afghanistan att...\n",
       "4            AskIndia  list india related sub reddits since removal d..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(['title', 'body'],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearranging the columns and writing the final dataset used for models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flair', 'input_features']\n",
      "['input_features', 'flair']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_features</th>\n",
       "      <th>flair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dat tweet .</td>\n",
       "      <td>Business/Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mom started kickstarter show early indian phot...</td>\n",
       "      <td>Photography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>please consider buying book india 2020 exodus ...</td>\n",
       "      <td>Science/Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>us double standards terrorists afghanistan att...</td>\n",
       "      <td>AskIndia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>list india related sub reddits since removal d...</td>\n",
       "      <td>AskIndia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      input_features               flair\n",
       "0                                        dat tweet .    Business/Finance\n",
       "1  mom started kickstarter show early indian phot...         Photography\n",
       "2  please consider buying book india 2020 exodus ...  Science/Technology\n",
       "3  us double standards terrorists afghanistan att...            AskIndia\n",
       "4  list india related sub reddits since removal d...            AskIndia"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = data.columns.tolist()\n",
    "print(cols)\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "print(cols)\n",
    "data = data[cols]\n",
    "\n",
    "data.to_csv('Data/final-data.csv', index=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train dim: (1409,) \ty_train dim: (1409,)\n",
      "x_test dim: (604,) \ty_test dim: (604,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input_features,y, test_size=0.3)\n",
    "\n",
    "print(\"x_train dim:\",x_train.shape, \"\\ty_train dim:\", y_train.shape)\n",
    "print(\"x_test dim:\",x_test.shape, \"\\ty_test dim:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "It is a typical classifier and most pervasively used one too. It has great interpretability properties.\n",
    "### Parameters:\n",
    "1. C = inverse regularization parameter; C = 1/λ;  It’s a penalty term meant regulate against overfitting.\n",
    "2. max_iter = The maximum number of passes over the training data, epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.28%\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          Politics       0.81      0.85      0.83        74\n",
      "     Non-Political       0.78      0.69      0.73        65\n",
      "       Coronavirus       0.93      0.97      0.95        69\n",
      "          AskIndia       0.92      0.82      0.87        60\n",
      "    Policy/Economy       0.92      0.95      0.94        62\n",
      "       Photography       0.68      0.73      0.70        66\n",
      "  Business/Finance       0.73      0.80      0.76        70\n",
      "            Sports       0.77      0.73      0.75        64\n",
      "Science/Technology       0.90      0.85      0.88        74\n",
      "\n",
      "          accuracy                           0.82       604\n",
      "         macro avg       0.83      0.82      0.82       604\n",
      "      weighted avg       0.83      0.82      0.82       604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "logistic = Pipeline([('cv', CountVectorizer()),('tfidf', TfidfTransformer()),('lr', LogisticRegression(C=1000, max_iter=1000))])\n",
    "logistic.fit(x_train, y_train)\n",
    "\n",
    "y_pred = logistic.predict(x_test)\n",
    "\n",
    "accuracy = (accuracy_score(y_pred, y_test)*100)\n",
    "print(\"Accuracy: %.2f\" % accuracy +'%')\n",
    "print(classification_report(y_test, y_pred,target_names=top_flairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference from Logistic Regression:\n",
    "1. The model has a total accuracy of **83%**, which is good.\n",
    "2. The most performing flair category was **\"Coronavirus\"**.\n",
    "3. The least performing flair categories was **\"Non-Political\"** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAIVE BAYES CLASSIFIER\n",
    "One of the most suitable variants for text is the multinomial variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.04%\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          Politics       0.73      0.51      0.60        74\n",
      "     Non-Political       0.40      0.78      0.53        65\n",
      "       Coronavirus       0.83      0.97      0.89        69\n",
      "          AskIndia       0.89      0.57      0.69        60\n",
      "    Policy/Economy       0.85      0.90      0.88        62\n",
      "       Photography       0.69      0.50      0.58        66\n",
      "  Business/Finance       0.72      0.59      0.65        70\n",
      "            Sports       0.56      0.70      0.62        64\n",
      "Science/Technology       0.98      0.70      0.82        74\n",
      "\n",
      "          accuracy                           0.69       604\n",
      "         macro avg       0.74      0.69      0.70       604\n",
      "      weighted avg       0.74      0.69      0.70       604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "naive = Pipeline([('cv', CountVectorizer()),('tfidf', TfidfTransformer()),('nb', MultinomialNB())])\n",
    "naive.fit(x_train, y_train)\n",
    "y_pred = naive.predict(x_test)\n",
    "\n",
    "accuracy = (accuracy_score(y_pred, y_test)*100)\n",
    "print(\"Accuracy: %.2f\" % accuracy +'%')\n",
    "print(classification_report(y_test, y_pred,target_names=top_flairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference from Naive Bayes Classifier:\n",
    "1. The model has a total accuracy of **69%**, which is decent.\n",
    "2. The most performing flair category was **\"Coronavirus\"**.\n",
    "3. The least performing flair category was **\"Non-Political\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "Random forest classifier suits most multi-class classification problem, also they have good interpretability and work faster.\n",
    "### Parameters:\n",
    "1. n_estimators = the number of trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.28%\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          Politics       0.86      0.91      0.88        74\n",
      "     Non-Political       0.82      0.72      0.77        65\n",
      "       Coronavirus       0.85      0.97      0.91        69\n",
      "          AskIndia       0.98      0.97      0.97        60\n",
      "    Policy/Economy       0.95      0.95      0.95        62\n",
      "       Photography       0.76      0.68      0.72        66\n",
      "  Business/Finance       0.53      0.74      0.62        70\n",
      "            Sports       0.84      0.72      0.77        64\n",
      "Science/Technology       1.00      0.76      0.86        74\n",
      "\n",
      "          accuracy                           0.82       604\n",
      "         macro avg       0.84      0.82      0.83       604\n",
      "      weighted avg       0.84      0.82      0.83       604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "random = Pipeline([('cv', CountVectorizer()),('tfidf', TfidfTransformer()),('rf', RandomForestClassifier(n_estimators = 600))])\n",
    "random.fit(x_train, y_train)\n",
    "\n",
    "y_pred = random.predict(x_test)\n",
    "\n",
    "accuracy = (accuracy_score(y_pred, y_test)*100)\n",
    "print(\"Accuracy: %.2f\" % accuracy +'%')\n",
    "print(classification_report(y_test, y_pred,target_names=top_flairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference from Random Forest Classifier:\n",
    "1. The model has a total accuracy of **82%**, which is goood.\n",
    "2. The most performing flair category was **\"AskIndia\"**.\n",
    "3. The least performing flair category was **\"Business/Finance\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pickle_files/random-forest.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(random, 'Pickle_files/random-forest.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k- Nearest Neighbours Classifier\n",
    "### Parameters:\n",
    "1. n_neighbours = number of neighbours to take into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.58%\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          Politics       0.70      0.43      0.53        74\n",
      "     Non-Political       0.49      0.65      0.56        65\n",
      "       Coronavirus       0.70      0.94      0.80        69\n",
      "          AskIndia       0.68      0.63      0.66        60\n",
      "    Policy/Economy       0.73      0.87      0.79        62\n",
      "       Photography       0.44      0.53      0.48        66\n",
      "  Business/Finance       0.59      0.41      0.49        70\n",
      "            Sports       0.51      0.52      0.51        64\n",
      "Science/Technology       0.89      0.68      0.77        74\n",
      "\n",
      "          accuracy                           0.63       604\n",
      "         macro avg       0.64      0.63      0.62       604\n",
      "      weighted avg       0.64      0.63      0.62       604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# k-Nearest Neighbours\n",
    "neighbours = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('knn', KNeighborsClassifier(n_neighbors=10))])\n",
    "neighbours.fit(x_train, y_train)\n",
    "y_pred = neighbours.predict(x_test)\n",
    "\n",
    "accuracy = (accuracy_score(y_pred, y_test)*100)\n",
    "print(\"Accuracy: %.2f\" % accuracy +'%')\n",
    "print(classification_report(y_test, y_pred,target_names=top_flairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference from k- Nearest Neighbours Classifier:\n",
    "1. The model has a total accuracy of **63%**, which is goood.\n",
    "2. The most performing flair category was **\"Coronavirus\"**.\n",
    "3. The least performing flair category was **\"Photography\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Machine\n",
    "### Parameters:\n",
    "1. loss = 'hinge' : The loss function to be used.‘hinge’ gives a linear SVM.\n",
    "2. penalyty = regularisation term. 'l2' for linear SVM.\n",
    "2. alpha = Constant that multiples the regularisation term.\n",
    "3. max_iter = The maximum number of passes over the training data, epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.77%\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          Politics       0.86      0.91      0.88        74\n",
      "     Non-Political       0.77      0.71      0.74        65\n",
      "       Coronavirus       0.83      0.97      0.89        69\n",
      "          AskIndia       0.93      0.90      0.92        60\n",
      "    Policy/Economy       0.91      0.98      0.95        62\n",
      "       Photography       0.74      0.73      0.73        66\n",
      "  Business/Finance       0.77      0.73      0.75        70\n",
      "            Sports       0.80      0.77      0.78        64\n",
      "Science/Technology       0.93      0.85      0.89        74\n",
      "\n",
      "          accuracy                           0.84       604\n",
      "         macro avg       0.84      0.84      0.84       604\n",
      "      weighted avg       0.84      0.84      0.84       604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = Pipeline([('cv', CountVectorizer()),('tfidf', TfidfTransformer()),('svm', SGDClassifier(loss='hinge', \n",
    "                penalty='l2',alpha=0.001, max_iter=20))])\n",
    "svm.fit(x_train, y_train)\n",
    "y_pred = svm.predict(x_test)\n",
    "\n",
    "accuracy = (accuracy_score(y_pred, y_test)*100)\n",
    "print(\"Accuracy: %.2f\" % accuracy +'%')\n",
    "print(classification_report(y_test, y_pred,target_names=top_flairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference from Linear SVM Classifier:\n",
    "1. The model has a total accuracy of **84%**, which is gooood.\n",
    "2. The most performing flair category was **\"Policy/Economy\"**.\n",
    "3. The least performing flair category was **\"Photography\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pickle_files/linear-svm.pkl']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(random, 'Pickle_files/linear-svm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUT OF THE 5 MODELS USED, LINEAR SVM PERFORMED THE BEST WITH A TOTAL AN ACCURACY OF 84%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
